{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from pyswarm import pso\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import os \n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from utils_AE import LRScheduler\n",
    "import umap\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "\n",
    "path = \"/root/workdir/IMMUCan/Files_TMA/Features_AE_raw/Milan_Hist_rawFeaturesConvAE10_10_64_split80-20.pickle\"\n",
    "        \n",
    "#path = \"/root/workdir/IMMUCan/Files_TMA/Features_AE_raw/Milan_Hist_rawFeatures60CropsConvAE_z3_3_64_split80-20.pickle\"\n",
    "\n",
    "with open(path,\"rb\",) as archivo:\n",
    "    features = pickle.load(archivo)\n",
    "\n",
    "if \"data\" not in features.keys():\n",
    "    folds = {\"data\":{\"fold0\":features}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \"SVM\" #\"SVM\" \"RF\" \"MLP\"\n",
    "augmentation = True\n",
    "components  = 10\n",
    "reduction = \"PLS\" # \"PCA\" \"PLS\"  \"LDA\" \"umap\"\n",
    "rcrop = None #60 \n",
    "\n",
    "if classifier == \"MLP\":\n",
    "    lr=0.01\n",
    "    epochs = 200\n",
    "    LrScheduler = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=228):\n",
    "    \"\"\"Function used to manage same seed and therefore reproducibility\"\"\"\n",
    "    print(f\"=>[REPLICABLE] True, with seed {seed}\")\n",
    "    print(\"    =>WARNING: SLOWER THIS WAY\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if classifier == \"MLP\":\n",
    "    seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE for Data Augmentation\n",
      "\n",
      " fold0 \n",
      "\n",
      "train set:  (117, 6400)\n",
      "val set:  (29, 6400)\n",
      "reduction with PLS\n",
      "new shape train:  (152, 10)\n",
      "new shape val:  (29, 10)\n",
      "Classifier:  SVM\n",
      "acc:  0.896551724137931\n",
      "f1:  0.7999999999999999\n",
      "[[20  1]\n",
      " [ 2  6]]\n",
      "Sensitivity :  0.9523809523809523\n",
      "Specificity :  0.75\n",
      "AUC:  0.8511904761904762\n"
     ]
    }
   ],
   "source": [
    "if augmentation:\n",
    "    print(\"SMOTE for Data Augmentation\")\n",
    "    \n",
    "if classifier == \"RF\":\n",
    "    #clf = RandomForestClassifier(n_estimators=200, max_depth=5, max_leaf_nodes=5, random_state=0)\n",
    "    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "elif classifier == \"SVM\":\n",
    "    kernel = 1.0 * RBF(1.0)    \n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma=1e1, C=1e1, kernel=kernel))\n",
    "    \n",
    "\n",
    "if classifier == \"MLP\":    \n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_size,  output_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, int(input_size*0.5))  \n",
    "            self.bn1 = nn.BatchNorm1d(int(input_size*0.5))\n",
    "            self.relu = nn.ReLU()  \n",
    "            self.fc2 = nn.Linear(int(input_size*0.5), int(input_size*0.25))  \n",
    "            self.bn2 = nn.BatchNorm1d(int(input_size*0.25))\n",
    "            self.fc3 = nn.Linear(int(input_size*0.25), output_size)            \n",
    "            self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)   \n",
    "            x = self.bn1(x)         \n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.bn2(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc3(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "      \n",
    "    \n",
    "##### Train model\n",
    "scores = []\n",
    "for f in folds[\"data\"]:\n",
    "    print(\"\\n\", f, \"\\n\")\n",
    "    X_train = folds[\"data\"][f][\"train\"][\"train_features\"]\n",
    "    X_val = folds[\"data\"][f][\"val\"][\"val_features\"]\n",
    "    val_names = folds[\"data\"][f][\"val\"]['val_names']\n",
    "    print(\"train set: \",X_train.shape)\n",
    "    print(\"val set: \",X_val.shape)\n",
    "\n",
    "        \n",
    "    y_train = folds[\"data\"][f][\"train\"][\"train_y\"]\n",
    "    y_val = folds[\"data\"][f][\"val\"][\"val_y\"]\n",
    "    val_names = features[\"val\"][\"val_names\"]\n",
    "\n",
    "    ## augment data\n",
    "\n",
    "    if augmentation:\n",
    "        sm = BorderlineSMOTE(sampling_strategy='all',k_neighbors=5,random_state = 42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    ## dimensionality reduction\n",
    "    if reduction == \"PCA\":\n",
    "        pca = PCA(n_components=components)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_val = pca.fit_transform(X_val)\n",
    "    elif reduction == \"LDA\":\n",
    "        print(\"reduction with LDA\")\n",
    "        components = 1\n",
    "        lda = LinearDiscriminantAnalysis(n_components=components)\n",
    "        X_train = lda.fit_transform(X_train, y_train)\n",
    "        X_val = lda.transform(X_val)\n",
    "        print(\"train: \" ,X_train.shape)\n",
    "        print(\"val: \" ,X_val.shape)\n",
    "    elif reduction == \"PLS\":\n",
    "        print(\"reduction with PLS\")\n",
    "        pls = PLSRegression(n_components=components)\n",
    "        pls.fit(X_train,y_train)\n",
    "        X_train = pls.transform(X_train)\n",
    "        X_val = pls.transform(X_val)\n",
    "    elif reduction == \"umap\":\n",
    "        print(\"reduction with UMAP\")\n",
    "        umap_model  = umap.UMAP(n_neighbors=10,min_dist=0.5,)\n",
    "        X_train = umap_model.fit_transform(X_train)\n",
    "        X_val = umap_model.transform(X_val)\n",
    "           \n",
    "    else:\n",
    "        components=X_train.shape[1]\n",
    "\n",
    "    print(\"new shape train: \",X_train.shape)\n",
    "    print(\"new shape val: \",X_val.shape)\n",
    "\n",
    "    if classifier == \"RF\" or classifier == \"SVM\":\n",
    "        print(\"Classifier: \",classifier)\n",
    "        clf.fit(X_train, y_train)        \n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        if rcrop:\n",
    "            print(\"\\n\",\"Majority voting with\", rcrop,\" crops\",\"\\n\")\n",
    "            y_pred = torch.tensor(y_pred)\n",
    "            y_val = torch.tensor(y_val)\n",
    "            n_pred = []\n",
    "            n_lab = []\n",
    "            n_name = []\n",
    "            percent = []\n",
    "            for bp in range(int(len(y_pred) / rcrop)):\n",
    "                ## majority voting\n",
    "                n_pred.append(\n",
    "                    torch.mode(\n",
    "                        y_pred[\n",
    "                            bp * rcrop : (bp + 1) * rcrop\n",
    "                        ].squeeze()\n",
    "                    ).values.item()\n",
    "                )\n",
    "                ### percentage\n",
    "                per0 = (sum(y_pred[\n",
    "                            bp * rcrop : (bp + 1) * rcrop\n",
    "                        ].squeeze())*100)/rcrop\n",
    "                per1 = (rcrop - sum(y_pred[\n",
    "                            bp * rcrop : (bp + 1) * rcrop\n",
    "                        ].squeeze()))*100 /rcrop\n",
    "                if per0>per1:\n",
    "                    percent.append(per0.item())\n",
    "                else:\n",
    "                    percent.append(per1.item())\n",
    "               \n",
    "                \"\"\"# If at least 1 crop predicted as 1, the gloabl patient labels is 1\n",
    "                n_pred.append(\n",
    "                    int(\n",
    "                        torch.any(\n",
    "                            binary_predictions[\n",
    "                                bp * n_crops : (bp + 1) * n_crops\n",
    "                            ].squeeze()\n",
    "                            == 1\n",
    "                        )\n",
    "                    )\n",
    "                )\"\"\"\n",
    "                n_lab.append(\n",
    "                    y_val[bp * rcrop : (bp + 1) * rcrop][0].item()\n",
    "                )\n",
    "                n_name.append(val_names[bp * rcrop : (bp + 1) * rcrop][0])\n",
    "\n",
    "            y_val = n_lab\n",
    "            y_pred = n_pred\n",
    "\n",
    "        if rcrop:\n",
    "            correct = (\n",
    "                sum(np.array(y_pred)==np.array(y_val))\n",
    "            )\n",
    "            accuracy = correct / len(y_val)\n",
    "            cm = confusion_matrix(y_val,y_pred)\n",
    "            fpr, tpr, thresholds = roc_curve(y_val,y_pred)\n",
    "        else:\n",
    "            accuracy = clf.score(X_val, y_val)\n",
    "            cm = confusion_matrix(y_val,clf.predict(X_val))\n",
    "            fpr, tpr, thresholds = roc_curve(y_val,clf.predict(X_val))\n",
    "\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        print(\"acc: \",accuracy)\n",
    "        print(\"f1: \",f1)\n",
    "        scores.append(f1)\n",
    "\n",
    "        \n",
    "        print(cm)\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        print(\"Sensitivity : \", sensitivity)\n",
    "\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(\"Specificity : \", specificity)\n",
    "\n",
    "        \n",
    "        roc_auc = auc(fpr, tpr)            \n",
    "        print(\"AUC: \",roc_auc)\n",
    "\n",
    "    if classifier == \"MLP\":\n",
    "        clf = MLP(input_size=components, output_size=2)\n",
    "        optimizer = optim.Adam(clf.parameters(), lr=lr)\n",
    "        if LrScheduler:\n",
    "            lr_scheduler = LRScheduler(optimizer,patience=5)\n",
    "        \n",
    "        X_train, y_train = torch.Tensor(X_train), torch.LongTensor(y_train)\n",
    "        X_val, y_val = torch.Tensor(X_val), torch.LongTensor(y_val)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "        \n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        acc = []\n",
    "        lval = []\n",
    "        for epoch in range(epochs):\n",
    "            clf.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = clf(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(train_dataset)\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "    \n",
    "            clf.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                total_loss = 0\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    outputs = clf(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    total_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_y.size(0)\n",
    "                    correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            average_loss = total_loss / len(val_dataset)\n",
    "            lval.append(average_loss)\n",
    "            acc.append(accuracy)\n",
    "            print(f'Accuracy on validation set: {accuracy:.2f}%')\n",
    "\n",
    "            if LrScheduler:\n",
    "                lr_scheduler(average_loss)\n",
    "\n",
    "        plt.plot(range(len(acc)), acc, label='val accuracy')\n",
    "        plt.title(\"val acc\")\n",
    "        plt.ylim(50,100)\n",
    "        plt.show()\n",
    "        plt.plot(range(len(lval)), lval, label='val loss')\n",
    "        plt.title(\"val loss\")\n",
    "        plt.show()\n",
    "        f1 = f1_score(y_val, predicted)        \n",
    "        print(\"f1: \",f1)        \n",
    "\n",
    "        cm = confusion_matrix(y_val,predicted)\n",
    "        print(cm)\n",
    "        sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "        print(\"Sensitivity : \", sensitivity)\n",
    "\n",
    "        specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "        print(\"Specificity : \", specificity)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_val,predicted)\n",
    "        roc_auc = auc(fpr, tpr)            \n",
    "        print(\"AUC: \",roc_auc)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
